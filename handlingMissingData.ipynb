{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0102292416367\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "import numpy\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "dataset = read_csv('TestDataSet.csv', header=None)\n",
    "# split dataset into inputs and outputs\n",
    "values = dataset.values\n",
    "X = values[:,0:3]\n",
    "y = values[:,3]\n",
    "# fill missing values with mean column values\n",
    "imputer = Imputer()\n",
    "transformed_X = imputer.fit_transform(X)\n",
    "# evaluate an LDA model on the dataset using k-fold cross validation\n",
    "model = LinearDiscriminantAnalysis()\n",
    "kfold = KFold(n_splits=4, random_state=7)\n",
    "result = cross_val_score(model, transformed_X, y, cv=kfold, scoring='accuracy')\n",
    "print(result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.decomposition import PCA\n",
    "from pandas import ExcelFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imputer() Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for ^: 'numpy.float64' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-38cf1d88ae93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtestrange2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m489\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0msse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestrange1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtestrange2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m^\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for ^: 'numpy.float64' and 'int'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('TestDataSet.csv')\n",
    "data1 = pd.read_csv('housing.csv')\n",
    "data2 = data1.as_matrix()\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=1)\n",
    "new_df = imp.fit_transform(data)\n",
    "\n",
    "testrange1 = new_df[:, 0:3]\n",
    "testrange2 = data2[1:489, 0:3]\n",
    "\n",
    "sse = sum(testrange1[:,1] - testrange2[:,1])^2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class imputation(object):\n",
    "    def __init__(self, dataframe, strategy, axis):\n",
    "        self.dataframe = dataframe\n",
    "        self.strategy = strategy\n",
    "        self.axis = axis\n",
    "    \n",
    "    def imputer(self):\n",
    "            # axis 0 = impute by column \n",
    "            # axis 1 = impute by row\n",
    "            # strategy takes \"mean\", \"median\", or \"most_frequent\" \n",
    "            imp = Imputer(missing_values='NaN', strategy=self.strategy, axis=self.axis)\n",
    "\n",
    "            # transformed dataset after imputation \n",
    "            new_df = imp.fit_transform(self.dataframe)\n",
    "\n",
    "            return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Zeroth Order Categorical Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class drop(object):\n",
    "    def __init__(self, dataframe, axis, how, method, value):\n",
    "        self.dataframe = dataframe\n",
    "        self.axis = axis\n",
    "        self.how = how\n",
    "        self.method = method\n",
    "        self.value = value\n",
    "\n",
    "        # check for null in dataset. Returns boolean\n",
    "        def naCheck(outcome1):\n",
    "            outcome1 = self.dataframe.isnull()\n",
    "            return outcome1\n",
    "\n",
    "        # drops row\n",
    "        def dropNArow(outcome2): \n",
    "            # how --> any : if any NA values are present, drop that label\n",
    "            # how --> all : if all values are NA, drop that label\n",
    "            outcome2 = self.dataframe.dropna(axis=0, how=self.how, thresh=None, subset=None, inplace=False)\n",
    "            return outcome2\n",
    "\n",
    "        # drops column\n",
    "        def dropNAcol(outcome3):\n",
    "            # any : if any NA values are present, drop that label\n",
    "            # all : if all values are NA, drop that label\n",
    "            outcome3 = self.dataframe.dropna(axis=1, how=self.how, thresh=None, subset=None, inplace=False)\n",
    "            return outcome3\n",
    "\n",
    "        # value : scalar or dict\n",
    "        # Value to use to fill holes (e.g. 0), alternately a dict of values specifying which value to use for each column (columns not in the dict will not be filled)\n",
    "        def fillNAzero(outcome4):\n",
    "            outcome4 = self.dataframe.fillna(value=self.value, method=self.method, axis=self.axis, inplace=False, limit=None, downcast=None)\n",
    "            return outcome4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/allentran/pca-magic.git\n",
      "  Cloning https://github.com/allentran/pca-magic.git to c:\\users\\zuf\\appdata\\local\\temp\\pip-obsvq8-build\n",
      "Requirement already satisfied: numpy in c:\\users\\zuf\\anaconda2\\lib\\site-packages (from ppca==0.0.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\zuf\\anaconda2\\lib\\site-packages (from ppca==0.0.3)\n",
      "Installing collected packages: ppca\n",
      "  Running setup.py install for ppca: started\n",
      "    Running setup.py install for ppca: finished with status 'done'\n",
      "Successfully installed ppca-0.0.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/allentran/pca-magic.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ppca import PPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426L, 4L)\n",
      "[[-0.5015889   0.40358776 -0.70618185  0.29467387]\n",
      " [ 0.5301662  -0.20821484 -0.68506973 -0.45414738]\n",
      " [ 0.37778782  0.88998669  0.15591914 -0.20221098]\n",
      " [-0.56974446  0.04107546  0.0876121  -0.8161055 ]]\n",
      "[[ -1.66342335e+05   1.32265835e+05   8.30720238e+04  -6.29274648e+10]\n",
      " [ -2.67234926e+05   2.12413312e+05   1.33458780e+05  -1.01091044e+11]\n",
      " [ -2.57222974e+05   2.04467466e+05   1.28461312e+05  -9.73037317e+10]\n",
      " ..., \n",
      " [ -1.84056548e+05   1.46362774e+05   9.19227024e+04  -6.96278285e+10]\n",
      " [ -1.69423214e+05   1.34737982e+05   8.46146631e+04  -6.40926641e+10]\n",
      " [ -9.16366080e+04   7.29481381e+04   4.57707542e+04  -3.46686485e+10]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"TestDataSet.csv\") \n",
    "new_df= df.dropna(how='any', axis=0)\n",
    "data = df.as_matrix()\n",
    "new_data = new_df.as_matrix()\n",
    "nComp = 4\n",
    "ppca = PPCA()\n",
    "\n",
    "\n",
    "#print a\n",
    "#print df.shape\n",
    "#print df\n",
    "\n",
    "mu = new_data.mean(axis=0)\n",
    "\n",
    "#fitted = ppca.fit(data, n_components)\n",
    "fitted_new = ppca.fit(new_data, nComp)\n",
    "#transformed = ppca.transform(data)\n",
    "transformed_new = ppca.transform(new_data)\n",
    "print(transformed_new.shape)\n",
    "\n",
    "#reverse = transformed*ppca.stds+ ppca.means\n",
    "#print(reverse)\n",
    "\n",
    "#Xhat = np.dot(ppca.transform(data)[:,:nComp], ppca.data[:nComp,:])\n",
    "#Xhat_new = np.dot(ppca.transform(new_data)[:,:n_components], ppca.data[:n_components,:])\n",
    "#Xhat_new = np.dot(transformed_new[:,:n_components], ppca.data[:n_components, :])\n",
    "\n",
    "Xhat_new = np.dot(transformed_new[:,:nComp], ppca.C[:nComp,:])\n",
    "Xhat = np.dot(ppca.transform(new_data)[:,:nComp], ppca.C[:nComp,:])\n",
    "#print ppca.data.shape\n",
    "print ppca.C\n",
    "#print ppca.eig_vals\n",
    "\n",
    "print transformed_new[:,:nComp]*ppca.stds + ppca.means\n",
    "\n",
    "#print ppca.means, ppca.means.shape\n",
    "#print ppca.stds, ppca.stds.shape\n",
    "\n",
    "#print Xhat_new.shape\n",
    "#Xhat_new = np.dot(ppca.data[:n_components, :], transformed_new)\n",
    "#Xhat_new += mu[:nComp,]\n",
    "#print(data[1,])\n",
    "#print(ppca.raw[1,])\n",
    "#print(ppca.data[1,])\n",
    "#print(transformed_new[1,])\n",
    "#print(Xhat[1,])\n",
    "#print(Xhat_new)\n",
    "#print(Xhat[1,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df = df.dropna(axis=0)\n",
    "new_prices = new_df.iloc[:,3]\n",
    "new_features = new_df.iloc[:,0:3]\n",
    "\n",
    "na_df = df[df.isnull().any(axis=1)]\n",
    "na_features = na_df.iloc[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.185   4.03   17.8  ]\n",
      "[  7.185   4.03   17.8  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nXna = np.dot(pca.transform(na_features)[:,:nComp], pca.components_[:nComp:])\\nprint pca.components_[:nComp:]\\nXna += mu\\nprint(na_f[1,])\\nprint(Xna[1,])\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reverse PCA on complete set\n",
    "nComp = 3\n",
    "pca = PCA(nComp)\n",
    "pca.fit_transform(new_features)\n",
    "\n",
    "mu = new_features.mean(axis=0)\n",
    "mu = mu.as_matrix()\n",
    "\n",
    "nf = new_features.as_matrix()\n",
    "na_f = na_features.as_matrix()\n",
    "\n",
    "Xhat = np.dot(pca.transform(new_features)[:,:nComp], pca.components_[:nComp,:])\n",
    "#print new_df.shape\n",
    "#print pca.transform(new_features)[:,:nComp].shape\n",
    "#print pca.components_[:nComp,:]\n",
    "#print Xhat.shape\n",
    "Xhat += mu\n",
    "print(nf[1,])\n",
    "print(Xhat[1,])\n",
    "\n",
    "# reverse PCA on missing set\n",
    "\"\"\"\n",
    "Xna = np.dot(pca.transform(na_features)[:,:nComp], pca.components_[:nComp:])\n",
    "print pca.components_[:nComp:]\n",
    "Xna += mu\n",
    "print(na_f[1,])\n",
    "print(Xna[1,])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class pca(object):\n",
    "    def __init__(self, dataframe, x_features, y_feature, moe):\n",
    "        self.dataframe = dataframe\n",
    "        self.x_features = x_features\n",
    "        self.y_feature = y_feature\n",
    "        self.moe = moe\n",
    "    \n",
    "    def principal_component_analysis(self):\n",
    "        # do the imputation first to prepare for first iteration\n",
    "        imp = Imputer(missing_values='NaN', strategy=\"mean\", axis=self.axis)\n",
    "        new_df = imp.fit_transform(self.dataframe)\n",
    "        \n",
    "        # set default margin of error. If none, then go for default. Compare values within missing indices. Set extra arrays. \n",
    "        # Build a dictionary to chain index and values \n",
    "        \n",
    "        # un-PCA to compare changed values. then calculate MOE. \n",
    "        \n",
    "        # find NAN row. then find indices of good rows. PCA on good rows only (builds functional relationship). Then\n",
    "        # using PCA components to get actual principal components. Store those. Using NAN indices and prinicpal components to \n",
    "        # guess NAN. \n",
    "        \n",
    "        # toy dataset for mapping principal components to NAN after PCA. do this first. \n",
    "        \n",
    "        if not self.moe:\n",
    "            # margin of error = +/-5% \n",
    "            moe_pos = 0.05\n",
    "            for i in range(0,100):\n",
    "                pca = PCA(n_components=len(self.x_features), svd_solver='auto')\n",
    "                Xnew = pca.fit_transform(X=new_df[self.x_features])\n",
    "                dataframe = DataframeUtilities().array_to_dataframe(array=Xnew)\n",
    "                dataframe = FeatureIO(dataframe=dataframe).add_custom_features(features_to_add=[self.y_feature], data_to_add=new_df[self.y_feature])\n",
    "                \n",
    "                # calculates margin of error for each iteration\n",
    "                moe_calc = (dataframe[i+1]-dataframe[i])/dataframe[i]\n",
    "                \n",
    "                if moe_calc > moe_pos:\n",
    "                    \n",
    "        else \n",
    "            # calculates margin of error for each iteration\n",
    "            moe_calc = (dataframe[i+1]-dataframe[i])/dataframe[i]g\n",
    "                \n",
    "            if moe_calc > self.moe:\n",
    "                    \n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Machine Learning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "7\n",
      "13\n",
      "19\n",
      "25\n",
      "39\n",
      "45\n",
      "51\n",
      "57\n",
      "71\n",
      "77\n",
      "83\n",
      "89\n",
      "171.8099606\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the sample dataset\n",
    "data = pd.read_csv('MASTMLsample.csv')\n",
    "\n",
    "# extracting a sample column with NaN\n",
    "feature_original = data['Site2_ThirdIonizationEnergy']\n",
    "target_original = data['Stability']\n",
    "feat_mod = []\n",
    "ind_nan = []\n",
    "target_mod = []\n",
    "\n",
    "for i in feature_original:\n",
    "    if i != 'nan':\n",
    "        feat_mod.append(i)\n",
    "\n",
    "# array with NaN removed \n",
    "feat_mod = feature_original[~np.isnan(feature_original)]\n",
    "\n",
    "# row index of NaN array\n",
    "ind_nan = np.where(np.isnan(feature_original))\n",
    "\n",
    "# remove and reassign input array that has index corresponding to NaN indices\n",
    "for i in ind_nan:\n",
    "    print type(i)\n",
    "    for j in i:\n",
    "        print j\n",
    "        target_mod = target_original.pop(j)\n",
    "\n",
    "\n",
    "#print ind_nan\n",
    "print target_mod\n",
    "\n",
    "# target array used for predicting NaN fields\n",
    "#target = list(set(target_original) - set(target_mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing split was successful.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split as tts\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(feature_mod, target_mod, test_size=0.2)\n",
    "\n",
    "print \"Training and testing split was successful.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gets r2 score for regression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def performance_metric(y_true, y_predict):\n",
    "    score = r2_score(y_true, y_predict)\n",
    "    return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zuf\\Anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def fit_model(X, y):\n",
    "    \"\"\" Performs grid search over the 'max_depth' parameter for a decision tree regressor trained on the input data [X, y]. \"\"\"\n",
    "    \n",
    "    # Create cross-validation sets from the training data\n",
    "    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n",
    "    \n",
    "    # regressor object\n",
    "    regressor = DecisionTreeRegressor()\n",
    "    \n",
    "    \"\"\"dictionary for the parameter 'max_depth' with a range from 1 to 10.\n",
    "       max_depth: how many questions the decision tree algorithm is allowed to ask about the data before making a prediction\"\"\"\n",
    "    params = {\"max_depth\": range(1, 20)}\n",
    "    \n",
    "    # Make a scorer from a performance metric or loss function.\n",
    "    scoring_fnc = make_scorer(performance_metric)\n",
    "    \n",
    "    # the grid search cv object\n",
    "    grid = GridSearchCV(regressor, params, scoring = scoring_fnc, cv = cv_sets)\n",
    "\n",
    "    # Fit the grid search object to the data to compute the optimal model\n",
    "    grid = grid.fit(X, y)\n",
    "\n",
    "    # Return the optimal model after fitting the data\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26.79225   ,  26.79225   ,  36.6945    ,  26.79225   ,\n",
       "        26.79225   ,  26.98641667,  26.98641667,  36.6945    ,\n",
       "        26.98641667,  26.79225   ,  26.98641667,  26.98641667,\n",
       "        26.98641667,  26.98641667,  26.98641667,  26.98641667,\n",
       "        26.79225   ,  26.79225   ,  36.6945    ,  26.79225   ,\n",
       "        26.79225   ,  26.98641667,  26.98641667,  36.6945    ,\n",
       "        26.98641667,  26.98641667,  36.6945    ,  26.98641667,\n",
       "        26.79225   ,  26.79225   ,  36.6945    ,  26.79225   ,\n",
       "        26.79225   ,  26.79225   ,  36.6945    ,  26.79225   ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the training data to the model using grid search\n",
    "reg = fit_model(X_train, y_train)\n",
    "\n",
    "new_input = target.values.reshape(-1, 1)\n",
    "\n",
    "reg.predict(new_input)\n",
    "\n",
    "#for i, target in enumerate(reg.predict(new_input)):\n",
    "   # print target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
