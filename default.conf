[Data Setup]

    data_path = ./test_data.csv
    save_path = ./output
    #lwr_data_path = CD_LWR_clean8.csv
    #X = N(Cu), N(Ni), N(Mn), N(P), N(Si), N( C ), N(log(fluence), N(log(flux), N(Temp)
    X = x1, x2, x3
    y = sin(x)

    weights = True # whether or not the weights column in the dataset is applied which duplicates weighted data

[Models and Tests to Run]

    models = gkrr_model
    #test_cases = ErrorBias, DescriptorImportance, KFold_CV, LeaveOutAlloyCV, FullFit, FluenceFluxExtrapolation
    test_cases = KFold_CV, FullFit

#if some test files have different configuration setting than AllTests, you can make changes by adding a
#separate section
[LeaveOutAlloyCV]
save_path = ../DBTT/graphs/leaveoutAlloy/{}.png

[Model Parameters]

    [[dtr_model]]
    max_depth = 5
    min_samples_split = 2
    min_samples_leaf = 1
    split criterion = mse

    [[gkrr_model]]
    alpha = 0.00139
    coef0 = 1
    degree = 3
    gamma = 0.518
    kernel = rbf

    [[lkrr_model]]
    alpha = 0.00518
    gamma = 0.518
    kernel = laplacian

    [[randomforest_model]]
    estimators = 100
    max_depth = 5
    min_samples_split = 2
    min_samples_leaf = 1
    max_leaf_nodes = None
    jobs = 1

    [[adaboost_model]]
    estimators = 275
    max_depth = 12
    min_samples_split = 2
    min_samples_leaf = 1
    learning rate = 1
    loss function = linear

    #minmax, size, transfer_function are the verbatim arguments for neurolab.net.newff()
    #training_algorithm is the verbatim 'support train fcn' for neurolab.train omitting 'train_'
    #see: https://pythonhosted.org/neurolab/lib.html#module-neurolab.net
    #epochs,show,goal are neurolab.net.train() arguments
    #see: https://pythonhosted.org/neurolab/lib.html#train-algorithms-based-gradients-algorithms
    #NOTE: minmax is verbose b/c [[0,1]]*9 will have bad pointers
    [[nn_model]]
    #minmax = [[0, 1],[0, 1],[0, 1],[0, 1],[0, 1],[0, 1],[0, 1],[0, 1],[0, 1]]
    minmax = [0, 1], [0, 1], [0, 1]
    size = 11, 1
    transfer_function = TanSig
    training_algorithm = bfgs
    epochs = 5
    show = False
    goal = 0.01