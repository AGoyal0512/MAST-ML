
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Random leave-out versus leave-out-group cross-validation &#8212; MAterials Simulation Toolkit for Machine Learning (MAST-ML) 2.0 documentation</title>
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Making predictions by importing a previously fit model" href="2_mastml_tutorial_modelimport.html" />
    <link rel="prev" title="Hyperparameter optimization" href="2_mastml_tutorial_hyperparam.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="2_mastml_tutorial_modelimport.html" title="Making predictions by importing a previously fit model"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="2_mastml_tutorial_hyperparam.html" title="Hyperparameter optimization"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">MAterials Simulation Toolkit for Machine Learning (MAST-ML) 2.0 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="2_mastml_tutorial.html" accesskey="U">MAST-ML tutorial</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="random-leave-out-versus-leave-out-group-cross-validation">
<h1>Random leave-out versus leave-out-group cross-validation<a class="headerlink" href="#random-leave-out-versus-leave-out-group-cross-validation" title="Permalink to this headline">¶</a></h1>
<p>Here, we will use our selected feature set and optimized KernelRidge hyperparameters from the previous section to do a
new kind of cross-validation test: leave out group (LOG) CV. To do this, you will modify the alpha and gamma values in the
Models section, KernelRidge model in your input file. In addition, you can rename the selected.csv data file to a
new name, for example “example_data_selected.csv”, and use the path to this new data file for this new run, as we
will not be performing feature selection again (to save time).</p>
<p>We will compare these results to the results of LOG cross-validation with the random cross-validation. Our input data file
had a column called “Host element”. This is a natural grouping to use for this problem, as it is interesting to assess our
fits when training on a set of host elements and predicted the values of an entirely new host element set, without having
ever trained on that set. Modify your input file to match what is shown below. Note that we have commented out the sections
that we no longer want with the # symbol. You can either comment out the sections or remove them entirely.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">GeneralSetup</span><span class="p">]</span>
    <span class="n">input_features</span> <span class="o">=</span> <span class="n">Auto</span>
    <span class="n">input_target</span> <span class="o">=</span> <span class="n">Reduced</span> <span class="n">barrier</span> <span class="p">(</span><span class="n">eV</span><span class="p">)</span>
    <span class="n">randomizer</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">Auto</span>
    <span class="n">input_other</span> <span class="o">=</span> <span class="n">Host</span> <span class="n">element</span><span class="p">,</span> <span class="n">Solute</span> <span class="n">element</span><span class="p">,</span> <span class="n">predict_Pt</span>
    <span class="n">input_grouping</span> <span class="o">=</span> <span class="n">Host</span> <span class="n">element</span>

<span class="c1">#[DataCleaning]</span>
<span class="c1">#    cleaning_method = imputation</span>
<span class="c1">#    imputation_strategy = mean</span>

<span class="c1">#[FeatureGeneration]</span>
<span class="c1">#    [[Magpie]]</span>
<span class="c1">#        composition_feature = Solute element</span>

<span class="p">[</span><span class="n">FeatureNormalization</span><span class="p">]</span>
    <span class="p">[[</span><span class="n">StandardScaler</span><span class="p">]]</span>

<span class="c1">#[FeatureSelection]</span>
<span class="c1">#    [[SequentialFeatureSelector]]</span>
<span class="c1">#        estimator = KernelRidge_select</span>
<span class="c1">#        k_features = 20</span>

<span class="c1">#[LearningCurve]</span>
<span class="c1">#    estimator = KernelRidge_learn</span>
<span class="c1">#    cv = RepeatedKFold_learn</span>
<span class="c1">#    scoring = root_mean_squared_error</span>
<span class="c1">#    n_features_to_select = 20</span>
<span class="c1">#    selector_name = SelectKBest</span>

<span class="p">[</span><span class="n">Models</span><span class="p">]</span>
    <span class="p">[[</span><span class="n">KernelRidge</span><span class="p">]]</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="n">rbf</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.034</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.138</span>
    <span class="c1">#[[KernelRidge_select]]</span>
    <span class="c1">#    kernel = rbf</span>
    <span class="c1">#    alpha = 1</span>
    <span class="c1">#    gamma = 1</span>
    <span class="c1">#[[KernelRidge_learn]]</span>
    <span class="c1">#    kernel = rbf</span>
    <span class="c1">#    alpha = 1</span>
    <span class="c1">#    gamma = 1</span>

<span class="p">[</span><span class="n">DataSplits</span><span class="p">]</span>
    <span class="p">[[</span><span class="n">NoSplit</span><span class="p">]]</span>
    <span class="p">[[</span><span class="n">RepeatedKFold</span><span class="p">]]</span>
        <span class="n">n_splits</span> <span class="o">=</span> <span class="mi">5</span>
        <span class="n">n_repeats</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="c1">#[[RepeatedKFold_learn]]</span>
    <span class="c1">#    n_splits = 5</span>
    <span class="c1">#    n_repeats = 2</span>
    <span class="p">[[</span><span class="n">LeaveOneGroupOut</span><span class="p">]]</span>
        <span class="n">grouping_column</span> <span class="o">=</span> <span class="n">Host</span> <span class="n">element</span>

<span class="c1">#[HyperOpt]</span>
<span class="c1">#    [[GridSearch]]</span>
<span class="c1">#        estimator = KernelRidge</span>
<span class="c1">#        cv = RepeatedKFold</span>
<span class="c1">#        param_names = alpha ; gamma</span>
<span class="c1">#        param_values = -5 5 100 log ; -5 5 100 log</span>
<span class="c1">#        scoring = root_mean_squared_error</span>
</pre></div>
</div>
<p>The main new additions to this input file is under the General Setup section, where the parameter grouping_feature needs
to be added, and the addition of LeaveOutGroup to the DataSplits section.</p>
<p>By doing this run, we can assess the model fits resulting from the random cross-validation and the LOG cross-validation.</p>
<p>Random cross-validation:</p>
<img alt="_images/MASTMLtutorial_run7_1.png" src="_images/MASTMLtutorial_run7_1.png" />
<p>LOG cross-validation:</p>
<img alt="_images/MASTMLtutorial_run7_2.png" src="_images/MASTMLtutorial_run7_2.png" />
<p>We can immediately see the R-squared and errors are both worse for the LOG cross-validation test compared to the random
cross-validation test. This is likely because the LOG test is a more rigorous test of model extrapolation, because the test
scores in each case are for data for which host elements were never included in the training set. In addition, a minor
effect contributing to the reduced accuracy may be due to the fact that the model hyperparameters were optimized by evaluating
the root mean squared error for a random cross-validation test. If instead the parameters were optimized using the LOG test,
the resulting fits would likely be improved.</p>
<p>There are a couple additional plots that are usual output for a LOG test that are worth drawing attention to. The first
is a plot of each metric test value for each group. This enables one to quickly assess which groups perform better or worse
than others.</p>
<img alt="_images/MASTMLtutorial_run7_3.png" src="_images/MASTMLtutorial_run7_3.png" />
<p>In addition, the parity plots for each split are now plotted with symbols denoting each group, which can help assess clustering
of groups and goodness of fit on a per-group basis.</p>
<p>Training on all groups except Ag:</p>
<img alt="_images/MASTMLtutorial_run7_4.png" src="_images/MASTMLtutorial_run7_4.png" />
<p>Testing on just Ag as the left-out host element:</p>
<img alt="_images/MASTMLtutorial_run7_5.png" src="_images/MASTMLtutorial_run7_5.png" />
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="2_mastml_tutorial_modelimport.html" title="Making predictions by importing a previously fit model"
             >next</a> |</li>
        <li class="right" >
          <a href="2_mastml_tutorial_hyperparam.html" title="Hyperparameter optimization"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">MAterials Simulation Toolkit for Machine Learning (MAST-ML) 2.0 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="2_mastml_tutorial.html" >MAST-ML tutorial</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2018, University of Wisconsin-Madison Computational Materials Group.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.4.4.
    </div>
  </body>
</html>