# You run this with `$ python mastml.py settings.conf data.csv -o results/`
# Second example:   `$ python mastml.py input.conf compositions.csv -o Desktop/model-results/`

# Or you open the website, upload a csv, upload a conf file, and download the resulting zip file

# Sections and subsections are in CamelCase; parameters are in snake_case


[GeneralSetup]
    input_features = Auto # Defaults to all but last column (specifying Auto is same as omiting this option)
    #input_features = square_footage, crime_rate, year_built # you can specify which columns from the csv you'd like to keep
    target_feature = Auto # Defaults to last column


[FeatureNormalization]
    # You can specify which features to normalize with keywords `input` and `target`, or you can explicityly specify which features:
    #include = Auto # Normalize all features
    #include = input # Normalize just input features
    #include = feat1, feat4 # Just normalize feature1 and feature4 from csv
    #TODO: add all sklearn feature normalization routines, plus our own
    normalization_type = standardize # Either standardize or normalize (??)
    mean = 0
    stddev = 1
    # Or
    # min = 0
    # max = 1
    # I'm not sure the best way to organize this


[FeatureGeneration] # If you don't want to use a particular API, then omit or comment out that subsection.
                    # If you don't want to do any feature generation, then omit this entire section
    #TODO: add all sklearn feature generation routines, plus our own
    [[magpie]]
        api_key = 1234
    [[materials_project]]
        api_key = 1234
        on_missing = error # Optional arg, defaults to 'ignore'
    [[citrine]]
        api_key=1234
    [[custom]]
        area = length * width # create new columns in the dataframe using algebra on existing columns


[FeatureSelection]
    #regression scoring functions: f_regression, mutual_info_regression
    #classification scoring functions: chi2, f_classif, mutual_info_classif
    #TODO: add all sklearn feature selection routines, plus our own

    [[RemoveConstantFeatures]]
    [[PrincipleComponentAnalysis]]
    [SelectKBest]
        k = 3
        scoring = f_classif
    [[VarianceThreshold]]
        threshold = 0.2
    [[SelectPercentile]]
        percentile = 10 # percentage of features to keep
        scoring = chi2
    [[SelectFpr]] # Select features based on a false positive rate test.
    [[SelectFdr]] # Select features based on an estimated false discovery rate.
    [[SelectFwe]] # Select features based on family-wise error rate.
    [[GenericUnivariateSelect]] # Univariate feature selector with configurable mode.
    [[RFE]] # recursive feature elimination


[DataSplits] # mastml will train & test the model on each of the following data splits.
             # Omit a split to not use it.
             # Or list a split without args to use it with default args.
             # Almost every option has a default argument.

    #TODO: add all sklearn data split routines, plus our own

    # Classification metrics: accuracy, average_precision, f1, f1_micro, f1_macro, f1_weighted, f1_samples, neg_log_loss, precision, recall, roc_auc,
    # Regression metrics: explained_variance, neg_mean_absolute_error, neg_mean_squared_error, neg_mean_squared_log_error, neg_median_absolute_error, r2,

    [[NoSplit]] # Just train the model on the training data and test it on that same data
        scoring_metric = accuracy # could also by r^2, or other things. determines definition of 'best' for plotting
        plots = best, worst, best_vs_avg, avg # Plots to save from this run
        #plots = Auto # just use the default plots (alternative)
        stats = accuracy, precision, recall # The stats to collect on each run. These will be saved with plot
        #stats = Auto # just use the default stats (alternative)

    [[Randomize]] # Randomly remap x and y values (just to see how the model does with nonsense input

    [[KFoldRandomize]] # First randomize all the x-y matchings, then see how well the nonsense-model does on cross validation
        k = 5

    [[WithoutEachCluster]] # First, cluster the data, then do a run without each cluster
        cluster_features = input # use all input features, Or:
        #cluster_features = age, gender, height # just use these features to make clusters
        algorithm = kmeans
        num_clusters = auto # use default number of clusters for algorithm
        #num_clusters = 5  # directly say number of clusters desired (alternative)

    [[JustEachGroup]] # Train the model on one group at a time and test it on the rest of the data
        grouping_feature = class # The name of the column in the csv which contains classes
        plots = best, worst

    [[WithoutEachGroup]] # Train the model on (n-1)/n groups and test on the excluded group
        grouping_feature = class
        plots = avg_vs_worst

    [[WithoutElement]] # Train the model without each element, then test on the rows with that element
        composition_column = compositions # the name of the column in the csv containing the compositions
        element = C # For carbon, for example

    [[KFold]] # Split the data into k even chunks. Try training/testing on each chunk.
        k = 5

    [[LeaveOutPercent]] # Like KFold but percentage
        percentage = 20

    [[LeaveOneOut]] # Try training the model on all the data except one row, for every row


[Models] # List the sklearn models you want to use and the parameters you want to change.
         # Your models should be either all classifiers or all regressors. No mixing allowed.
         # All parameters have default settings. If you ommit a parameter, then the default setting is used.
         # Go to http://scikit-learn.org/stable/documentation.html and search for details on a particular model.
         # A single conf file should use zero or more models.
         # mastml will run each enabled split with each enabled model
         # Also check this out:
         # http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
     ### REGRESSORS

    [[SVR]]
        C
        cache_size
        coef0
        degree
        epsilon
        gamma
        kernel = rbf # or try: linear
        max_iter
        shrinking


    [[Lasso]]
        alpha
        copy_X
        fit_intercept
        max_iter
        normalize
        positive
        precompute
        random_state
        selection
        tol
        warm_start


    [[ElasticNet]]
        alpha
        copy_X
        fit_intercept
        l1_ratio
        max_iter
        normalize
        positive
        precompute
        random_state
        selection
        tol
        warm_start


    [[Ridge]]
        alpha
        copy_X
        fit_intercept
        max_iter
        normalize
        random_state
        solver
        tol


    [[LinearRegression]]
        copy_X
        fit_intercept
        n_jobs
        normalize

    [[DecisionTreeRegressor]]
        criterion
        max_depth
        max_features
        max_leaf_nodes
        min_impurity_decrease
        min_impurity_split
        min_samples_leaf
        min_samples_split
        min_weight_fraction_leaf
        presort
        random_state
        splitter

    [[KernelRidge]]
        alpha
        coef0
        degree
        gamma
        kernel
        kernel_params


    ### Classifiers

    [[MLPClassifier]]
        activation
        alpha
        batch_size
        beta_1
        beta_2
        early_stopping
        epsilon
        hidden_layer_sizes
        learning_rate
        learning_rate_init
        max_iter
        momentum
        nesterovs_momentum
        power_t
        random_state
        shuffle
        solver
        tol
        validation_fraction
        verbose
        warm_start


    [[KNeighborsClassifier]]
        algorithm
        leaf_size
        metric
        metric_params
        n_jobs
        n_neighbors
        p
        weights



    [[SVC]]
        C
        cache_size
        class_weight
        coef0
        decision_function_shape
        degree
        gamma
        kernel
        max_iter
        probability
        random_state
        shrinking
        tol
        verbose


    [[GaussianProcessClassifier]]
        copy_X_train
        kernel
        max_iter_predict
        multi_class
        n_jobs
        n_restarts_optimizer
        optimizer
        random_state
        warm_start


    [[DecisionTreeClassifier]]
        class_weight
        criterion
        max_depth
        max_features
        max_leaf_nodes
        min_impurity_decrease
        min_impurity_split
        min_samples_leaf
        min_samples_split
        min_weight_fraction_leaf
        presort
        random_state
        splitter


    [[RandomForestClassifier]]
        bootstrap
        class_weight
        criterion
        max_depth
        max_features
        max_leaf_nodes
        min_impurity_decrease
        min_impurity_split
        min_samples_leaf
        min_samples_split
        min_weight_fraction_leaf
        n_estimators
        n_jobs
        oob_score
        random_state
        verbose
        warm_start


    [[AdaBoostClassifier]]
        algorithm
        base_estimator
        learning_rate
        n_estimators
        random_state


    [[QuadraticDiscriminantAnalysis]]
        priors
        reg_param
        store_covariance
        store_covariances
        tol


[PlotSettings]
    basic_plots = True
    target_histogram = True
    TODOFIGURETHISOUTONOUT = True
    predicted_vs_true = True
    predicted_vs_true_bars = True
    best_worst_per_point = True

