[General Setup]

    input_features = Auto
    save_path = /Users/ryanjacobs/PycharmProjects/MASTML/unit_tests/
    target_feature = O_pband_center_regression
    config_files_path = /Users/ryanjacobs/PycharmProjects/MASTML/unit_tests/MASTML_config_files/

[Data Setup]

    [[Initial]]
    data_path = /Users/ryanjacobs/PycharmProjects/MASTML/unit_tests/testcsv1featureselection.csv
    weights = False

[Feature Normalization]
    normalize_x_features = True
    normalize_y_feature = False

[Feature Generation]
    perform_feature_generation = True
	add_magpie_features = True
	add_materialsproject_features = False
	materialsproject_apikey = 'TtAHFCrZhQa7cwEy' # Use your own Materials Project API key
	add_citrine_features = False
	citrine_apikey = 'amQVQutFrr7etr4ufQQh0gtt' # Use your own Citrine API key
	
[Feature Selection]
	perform_feature_selection = True
	remove_constant_features = True
	#feature_selection_algorithm = sequential_forward_selection
	feature_selection_algorithm = univariate_feature_selection
	#feature_selection_algorithm = recursive_feature_elimination
	use_mutual_information = False
	number_of_features_to_keep = 2
	scoring_metric = r2_score
	generate_feature_learning_curve = True
	model_to_use_for_learning_curve = gkrr_model_regressor

[Models and Tests to Run]

    models = linear_model_regressor, gkrr_model_regressor, adaboost_model_regressor
    test_cases = SingleFit, KFoldCV

[Test Parameters]

    [[SingleFit]]
    training_dataset = Initial
    testing_dataset  = Initial
    xlabel = Calculated O pband center_SINGLEFIT
    ylabel = Predicted O pband center_SINGLEFIT
    stepsize = 100
    
    [[KFoldCV]]
    training_dataset = Initial
    testing_dataset  = Initial
    xlabel = Calculated O pband center_KFOLD
    ylabel = Predicted O pband center_KFOLD
    stepsize = 100
    num_folds = 5
    num_cvtests = 20


[Model Parameters]

    [[linear_model_regressor]]
    fit_intercept = True

    [[linear_model_lasso_regressor]]
    alpha = 0.1
    fit_intercept = True,

    [[decision_tree_model_regressor]]
    max_depth = 5
    splitter = best
    min_samples_split = 2
    min_samples_leaf = 1
    criterion = mse

    [[gkrr_model_regressor]]
    alpha = 0.003556
    gamma = 0.001
    coef0 = 1
    degree = 3
    kernel = rbf

    [[lkrr_model_regressor]]
    alpha = 0.00518
    gamma = 0.518
    kernel = laplacian

    [[randomforest_model_regressor]]
    criterion = mse
    n_estimators = 50
    max_depth = 100
    min_samples_split = 2
    min_samples_leaf = 1
    max_leaf_nodes = 2
    n_jobs = 1
    warm_start = False

    [[extra_trees_model_regressor]]
    criterion = mse
    n_estimators = 100
    max_depth = 5
    min_samples_split = 2
    min_samples_leaf = 1
    max_leaf_nodes = 2

    [[adaboost_model_regressor]]
    base_estimator_max_depth = 5
    n_estimators = 100
    learning_rate = 1
    loss = linear

	[[nn_model_regressor]]
	hidden_layer_sizes = 10
	activation = relu # Choose relu, tanh, logistic, identity
	solver = adam # Choose adam, lbfgs, sgd
	alpha = 0.0001
	max_iterations = 200
	tolerance = 0.0001

	[[logistic_regression_model_classifier]]
	penalty = l2
	C = 0.1706
	class_weight = balanced # Choose between None, balanced

	[[support_vector_machine_model_classifier]]
	error_penalty = 1
	kernel = rbf # Choose between linear, rbf, logistic
	degree = 3
	gamma = 0.05
	coef0 = 1

	[[decision_tree_model_classifier]]
	criterion = entropy # Choose between gini, entropy
    splitter = best
    max_depth = 22
    min_samples_split = 2
    min_samples_leaf = 1

    [[random_forest_model_classifier]]
    criterion = entropy # Choose between gini, entropy
    n_estimators = 100
    max_depth = 5
    min_samples_split = 2
    min_samples_leaf = 1
    max_leaf_nodes = 2

	[[extra_trees_model_classifier]]
    criterion = entropy # Choose between gini, entropy
    n_estimators = 100
    max_depth = 5
    min_samples_split = 2
    min_samples_leaf = 1
    max_leaf_nodes = 2

    [[adaboost_model_classifier]]
    base_estimator_max_depth = 5
    n_estimators = 100
    learning_rate = 1

	[[nn_model_classifier]]
	hidden_layer_sizes = 10
	activation = relu # Choose relu, tanh, logistic, identity
	solver = adam # Choose adam, lbfgs, sgd
	alpha = 0.0001
	max_iterations = 200
	tolerance = 0.0001