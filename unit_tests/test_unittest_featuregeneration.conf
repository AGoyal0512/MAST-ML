[General Setup]

    input_features = Auto
    save_path = /Users/ryanjacobs/PycharmProjects/MASTML/unit_tests/
    target_feature = O_pband_center_regression
    config_files_path = /Users/ryanjacobs/PycharmProjects/MASTML/unit_tests/MASTML_config_files/

[Data Setup]

    [[Initial]]
    data_path = /Users/ryanjacobs/PycharmProjects/MASTML/unit_tests/testcsv1.csv
    weights = False

[Feature Normalization]
    normalize_x_features = False
    normalize_y_feature = False

[Feature Generation]
    perform_feature_generation = True
	add_magpie_features = True
	add_materialsproject_features = True
	materialsproject_apikey = 'TtAHFCrZhQa7cwEy' # Use your own Materials Project API key
	add_citrine_features = True
	citrine_apikey = 'amQVQutFrr7etr4ufQQh0gtt' # Use your own Citrine API key
	
[Feature Selection]
	perform_feature_selection = False
	remove_constant_features = False
	#feature_selection_algorithm = sequential_forward_selection
	feature_selection_algorithm = univariate_feature_selection
	#feature_selection_algorithm = recursive_feature_elimination
	use_mutual_information = False
	number_of_features_to_keep = 10
	scoring_metric = r2_score
	generate_feature_learning_curve = True
	model_to_use_for_learning_curve = gkrr_model_regressor

[Models and Tests to Run]

    models = gkrr_model_regressor
    test_cases = SingleFit, KFoldCV_5fold

[Test Parameters]

    [[SingleFit]]
    training_dataset = Initial
    testing_dataset  = Initial
    #xlabel = Calculated value feature1
    #ylabel = Predicted value feature1
    xlabel = Calculated O pband center_SINGLEFIT
    ylabel = Predicted O pband center_SINGLEFIT
    stepsize = 100
    
    [[KFoldCV_5fold]]
    training_dataset = Initial
    testing_dataset  = Initial
    #xlabel = Calculated value feature1
    #ylabel = Predicted value feature1
    xlabel = Calculated O pband center_KFOLD
    ylabel = Predicted O pband center_KFOLD
    stepsize = 100
    num_folds = 5
    num_cvtests = 20
    #fix_random_for_testing = 0
    #mark_outlying_points = 0,3

    [[SingleFit_predict]]
    training_dataset = Initial
    testing_dataset  = Predict
    xlabel = Calculated O pband center_FITPREDICT
    ylabel = Predicted O pband center_FITPREDICT
    stepsize = 100

    [[ParamOptGA]]
    training_dataset = Initial
    testing_dataset = Initial
    percent_leave_out = 20
    num_cvtests = 50
    mark_outlying_points = 0,3
    num_bests = 10
    processors = 2
    fix_random_for_testing = 0
    pop_upper_limit = 1000000
    num_gas = 5
    ga_pop_size = 50
    convergence_generations = 15
    max_generations = 200
    crossover_prob = 0.5
    mutation_prob = 0.1
    shift_prob = 0.5
    gen_tol = 0.00001
    param_1 = model;alpha;float;continuous-log;-10:0:50
    param_2 = model;gamma;float;continuous-log;-3:3:50
    #additional_feature_methods = DBTT.calculate_EffectiveFluence;1;flux_feature:flux_n_cm2_sec;fluence_feature:fluence_n_cm2

[Model Parameters]

    [[linear_model_regressor]]
    fit_intercept = True

    [[linear_model_lasso_regressor]]
    alpha = 0.1
    fit_intercept = True,

    [[decision_tree_model_regressor]]
    max_depth = 5
    splitter = best
    min_samples_split = 2
    min_samples_leaf = 1
    criterion = mse

    [[gkrr_model_regressor]]
    alpha = 0.003556
    gamma = 0.001
    coef0 = 1
    degree = 3
    kernel = rbf

    [[lkrr_model_regressor]]
    alpha = 0.00518
    gamma = 0.518
    kernel = laplacian

    [[randomforest_model_regressor]]
    criterion = mse
    n_estimators = 50
    max_depth = 100
    min_samples_split = 2
    min_samples_leaf = 1
    max_leaf_nodes = 2
    n_jobs = 1
    warm_start = False

    [[extra_trees_model_regressor]]
    criterion = mse
    n_estimators = 100
    max_depth = 5
    min_samples_split = 2
    min_samples_leaf = 1
    max_leaf_nodes = 2

    [[adaboost_model_regressor]]
    base_estimator_max_depth = 5
    n_estimators = 100
    learning_rate = 1
    loss = linear

	[[nn_model_regressor]]
	hidden_layer_sizes = 10
	activation = relu # Choose relu, tanh, logistic, identity
	solver = adam # Choose adam, lbfgs, sgd
	alpha = 0.0001
	max_iterations = 200
	tolerance = 0.0001

	[[logistic_regression_model_classifier]]
	penalty = l2
	C = 0.1706
	class_weight = balanced # Choose between None, balanced

	[[support_vector_machine_model_classifier]]
	error_penalty = 1
	kernel = rbf # Choose between linear, rbf, logistic
	degree = 3
	gamma = 0.05
	coef0 = 1

	[[decision_tree_model_classifier]]
	criterion = entropy # Choose between gini, entropy
    splitter = best
    max_depth = 22
    min_samples_split = 2
    min_samples_leaf = 1

    [[random_forest_model_classifier]]
    criterion = entropy # Choose between gini, entropy
    n_estimators = 100
    max_depth = 5
    min_samples_split = 2
    min_samples_leaf = 1
    max_leaf_nodes = 2

	[[extra_trees_model_classifier]]
    criterion = entropy # Choose between gini, entropy
    n_estimators = 100
    max_depth = 5
    min_samples_split = 2
    min_samples_leaf = 1
    max_leaf_nodes = 2

    [[adaboost_model_classifier]]
    base_estimator_max_depth = 5
    n_estimators = 100
    learning_rate = 1

	[[nn_model_classifier]]
	hidden_layer_sizes = 10
	activation = relu # Choose relu, tanh, logistic, identity
	solver = adam # Choose adam, lbfgs, sgd
	alpha = 0.0001
	max_iterations = 200
	tolerance = 0.0001