{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10523,
     "status": "ok",
     "timestamp": 1614624209059,
     "user": {
      "displayName": "Ryan Jacobs",
      "photoUrl": "",
      "userId": "08020825119070462259"
     },
     "user_tz": 360
    },
    "id": "xdEXmdzP7kWN",
    "outputId": "52f31504-19a9-4897-d4b5-39b27b3efaab"
   },
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "#\n",
    "#  Welcome to the fourth MAST-ML tutorial notebook, MASTML_Tutorial_4_Models_and_Tests.ipynb! \n",
    "#  In this notebook, we will learn how to run a few different types of models on a select\n",
    "#  dataset, and conduct a few different types of data splits to evaluate our model performance. In\n",
    "#  this tutorial, we will:\n",
    "#       1. Run a variety of model types from the scikit-learn package\n",
    "#       2. Run a bootstrapped ensemble of linear ridge regression models\n",
    "#       3. Compare performance of scikit-learn's gradient boosting method and XGBoost\n",
    "#       4. Compare performance of scikit-learn's neural network and Keras-based neural network regressor\n",
    "#       5. Compare model performance using random k-fold cross validation and leave out group\n",
    "#          cross validation\n",
    "#       6. Explore the limits of model performance when up to 90% of data is left out using\n",
    "#          leave out percent cross validation\n",
    "#\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#\n",
    "# Task 0: Setting up MAST-ML in Colab\n",
    "#\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are working on Google Colab and need to install MAST-ML, \n",
    "# begin by cloning the relevant branch of MAST-ML to the Colab session\n",
    "# and install the needed dependencies:\n",
    "\n",
    "!git clone --single-branch --branch dev_Ryan_2020-12-21 https://github.com/uw-cmg/MAST-ML\n",
    "!pip install -r MAST-ML/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23965,
     "status": "ok",
     "timestamp": 1614624307314,
     "user": {
      "displayName": "Ryan Jacobs",
      "photoUrl": "",
      "userId": "08020825119070462259"
     },
     "user_tz": 360
    },
    "id": "w-jHgEAFquSh",
    "outputId": "3a745bff-5d3b-4f64-df23-7095a8d0c670"
   },
   "outputs": [],
   "source": [
    "# Sync your Google drive to Colab so that we can save MAST-ML results to our Google\n",
    "# Drive. If we save to the Colab session, the data will be deleted when the session \n",
    "# ends.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1614624313213,
     "user": {
      "displayName": "Ryan Jacobs",
      "photoUrl": "",
      "userId": "08020825119070462259"
     },
     "user_tz": 360
    },
    "id": "nwQO48j1ws3S"
   },
   "outputs": [],
   "source": [
    "# We need to add the MAST-ML folder to our sys path so that python can find the modules\n",
    "import sys\n",
    "sys.path.append('MAST-ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2336,
     "status": "ok",
     "timestamp": 1614624316133,
     "user": {
      "displayName": "Ryan Jacobs",
      "photoUrl": "",
      "userId": "08020825119070462259"
     },
     "user_tz": 360
    },
    "id": "x8GIei1qQoWL",
    "outputId": "6480ed4e-44d3-43c9-f5c3-1fb1c1063801"
   },
   "outputs": [],
   "source": [
    "# Here we import the MAST-ML modules used in this tutorial\n",
    "from mastml.mastml import Mastml\n",
    "from mastml.datasets import LocalDatasets\n",
    "from mastml.models import SklearnModel, EnsembleModel\n",
    "from mastml.preprocessing import SklearnPreprocessor\n",
    "from mastml.data_splitters import SklearnDataSplitter, NoSplit, LeaveOutPercent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1614624317207,
     "user": {
      "displayName": "Ryan Jacobs",
      "photoUrl": "",
      "userId": "08020825119070462259"
     },
     "user_tz": 360
    },
    "id": "Xf8MFHjlM7Oe",
    "outputId": "71bbe4eb-544e-40de-ef0d-b57e8f632841"
   },
   "outputs": [],
   "source": [
    "# Set the name of the savepath to save MAST-ML results to\n",
    "SAVEPATH = 'drive/MyDrive/MASTML_tutorial_4_Models_and_Tests'\n",
    "\n",
    "# Initialize the MAST-ML run, write savepath\n",
    "mastml = Mastml(savepath=SAVEPATH)\n",
    "savepath = mastml.get_savepath\n",
    "\n",
    "# When the above command is run, a new folder with the name designated SAVEPATH is created.\n",
    "# This is where all of the output for the current MAST-ML run will be saved to.\n",
    "# Note that you can perform multiple runs with the same folder name, and the current datetime\n",
    "# will be appended to the name so that no data is lost or overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "#\n",
    "# Task 1: Run a variety of model types from the scikit-learn package\n",
    "#\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this tutorial, we will again use the diffusion dataset that we examined in the \n",
    "# previous tutorial. Here, we use the LocalDatasets module to load in the diffusion dataset. \n",
    "\n",
    "# Need to denote the column name of the target (y-data)\n",
    "target = 'E_regression'\n",
    "\n",
    "# There are columns in the data file not used as features or target. We need to\n",
    "# list them here in the parameter extra_columns\n",
    "extra_columns = ['Material compositions 1', 'Material compositions 2']\n",
    "\n",
    "# Here, we make an instance of our LocalDatasets class. It needs a few parameters:\n",
    "#   file_path: where the data is stored\n",
    "#   target: the column name of the y-data\n",
    "#   extra_columns: list containing extra columns in the data file not used for fitting\n",
    "#   group_column: column name denoting group labels (only used for LeaveOutGroup CV)\n",
    "#   testdata_columns: column names denoting left-out data to evaluate using best\n",
    "#     model from CV tests. This is manual way to leave out data. Can also be done\n",
    "#     automatically using nested CV (we will do this in later tutorials)\n",
    "#   as_frame: whether to return data as dataframe. Want this to be true.\n",
    "d = LocalDatasets(file_path='../mastml/data/diffusion_data_selectfeatures.xlsx', #'MAST-ML/mastml/data/figshare_7418492/All_Model_Data.xlsx'\n",
    "                  target=target, \n",
    "                  extra_columns=extra_columns, \n",
    "                  group_column='Material compositions 1',\n",
    "                  testdata_columns=None,\n",
    "                  as_frame=True)\n",
    "\n",
    "# Load the data with the load_data() method\n",
    "data_dict = d.load_data()\n",
    "\n",
    "# Let's assign each data object to its respective name\n",
    "X = data_dict['X']\n",
    "y = data_dict['y']\n",
    "X_extra = data_dict['X_extra']\n",
    "groups = data_dict['groups']\n",
    "X_testdata = data_dict['X_testdata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the first part of this tutorial, we will assess the performance of a handful\n",
    "# of different types of regression models in scikit-learn using full fits and\n",
    "# random 5-fold cross validation. Let's try out the following models:\n",
    "#     1.) Linear regression\n",
    "#     2.) Kernel ridge regression with Gaussian kernel\n",
    "#     3.) Decision tree regressor\n",
    "#     4.) Random forest regressor\n",
    "#     5.) Neural network\n",
    "\n",
    "# To make these models, we use the SklearnModel class. The scikit-learn model name \n",
    "# can just be given as a string matching the model name in the \"model\" field. The \n",
    "# remaining arguments are the parameters to pass to the model. If no parameters are given, \n",
    "# default values are used.\n",
    "model_lin = SklearnModel(model='LinearRegression')\n",
    "model_gkrr = SklearnModel(model='KernelRidge', kernel='rbf')\n",
    "model_gpr = SklearnModel(model='GaussianProcessRegressor', kernel='ConstantKernel*RBF', n_restarts_optimizer=10)\n",
    "model_rf = SklearnModel(model='RandomForestRegressor', n_estimators=150)\n",
    "model_nn = SklearnModel(model='MLPRegressor', hidden_layer_sizes=(20, 10))\n",
    "\n",
    "# MAST-ML takes a list of the models as input.\n",
    "models = [model_lin, model_gkrr, model_gpr, model_rf, model_nn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As in the previous tutorial, we need to define our preprocessing function. \n",
    "# We are just going to use the basic StandardScaler in scikit-learn to normalize \n",
    "# each column to have mean zero and standard deviation of one.\n",
    "preprocessor = SklearnPreprocessor(preprocessor='StandardScaler', as_frame=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we list which metrics we want to evaluate. If none are given, MAST-ML\n",
    "# will default to evaulating just the root mean squared error. A complete list of \n",
    "# metrics can be obtained from calling Metrics()._metric_zoo() in metrics.py.\n",
    "metrics = ['r2_score', 'mean_absolute_error', 'root_mean_squared_error', 'rmse_over_stdev']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a first run, we want to define and run the case where no data split is \n",
    "# performed. This represents a full fit to all of the data. MAST-ML will automatically\n",
    "# fit all of our models in sequence.\n",
    "\n",
    "# Note that each splitter.evaluate() method has a verbosity tag- this tag controls \n",
    "# the extent of analysis output plotting that is performed. A value of 3 is the highest,\n",
    "# meaning that the most output is produced\n",
    "\n",
    "splitter = NoSplit()\n",
    "splitter.evaluate(X=X,\n",
    "                  y=y, \n",
    "                  models=models,\n",
    "                  preprocessor=preprocessor,\n",
    "                  metrics=metrics,\n",
    "                  savepath=savepath,\n",
    "                  X_extra=X_extra,\n",
    "                  plots = ['Scatter', 'Histogram'],\n",
    "                  verbosity=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we want to do something a bit more informative than simply fitting to \n",
    "# all of our data. We will do a random leave-out cross validation test (5-fold CV). \n",
    "# MAST-ML will output data and plots for each split as well as some more comprehensive analysis \n",
    "# performed over all splits. The saved model and preprocessor corresponding to the best \n",
    "# split will also be put in the data splitter parent directory, and this saved model\n",
    "# can be imported for use in future predictions \n",
    "\n",
    "splitter = SklearnDataSplitter(splitter='RepeatedKFold', n_repeats=1, n_splits=5)\n",
    "splitter.evaluate(X=X,\n",
    "                  y=y, \n",
    "                  models=models,\n",
    "                  preprocessor=preprocessor,\n",
    "                  metrics=metrics,\n",
    "                  plots=['Scatter', 'Histogram'],\n",
    "                  savepath=savepath,\n",
    "                  X_extra=X_extra,\n",
    "                  verbosity=3)\n",
    "\n",
    "# MAST-ML saves output with a folder format of ModelName_DataSplitterName_PreprocessorName_FeatureSelectorName_Datetime\n",
    "# Examine the output of each model for the 5-fold CV runs. You'll see there are folders corresponding to each\n",
    "# split that was performed. In each of these split folders, the X and y train/test splits are saved as spreadsheets,\n",
    "# as well as the preprocessed data used for fitting. In addition, data on the residuals, a residuals histogram, and \n",
    "# parity plots for both train and test data are provided. There are also some files pertaining to the estimated model\n",
    "# errors for certain models (GPR, Random forest), but we'll discuss those more in Tutorial 6. \n",
    "#\n",
    "# In the main data splitter directory, there are summary-level plots of residual histograms and data over all splits,\n",
    "# and train and test parity plots, again taken over all splits. There are a few varieties of parity plots that may\n",
    "# be of interest:\n",
    "#     parity_plot_allsplits_average_(train/test).png: This plot is pred vs. true values, one point per unique true data point. \n",
    "#         The average test value of each point is plotted, so leaving out the same point multiple times gives an error bar.\n",
    "#     parity_plot_best_worst_eachpoint_(train/test).png: Parity plot where each point is the best or worst prediction over all\n",
    "#         the splits for that particular data point.\n",
    "#     parity_plot_best_worst_split_(train/test).png: Parity plot where the data series pertain to the best and worst individual\n",
    "#         splits.\n",
    "#     parity_plot_(train/test).png: Parity plot showing all the data points from all splits, without averaging values for\n",
    "#         each unique true data point.\n",
    "#\n",
    "# Which model performed the best? I found that \n",
    "# GPR had the lowest average test RMSE of just 0.155 eV, with Random forest in close second at 0.161 eV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "#\n",
    "# Task 2: Run a bootstrapped ensemble of linear ridge regression models\n",
    "#\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just finished running a number of different types of models. One other type of \n",
    "# flexible and quite generally powerful model is a bootstrapped ensemble model. We\n",
    "# previously already ran one of these- the random forest, which is a bootstrapped ensemble\n",
    "# of decision trees. However, we can create other types of bootstrap ensembles models\n",
    "# using the EnsembleModel() in MAST-ML. Here, let's build an ensemble of linear ridge\n",
    "# regression models. We will explore the implications of using ensemble models for uncertainy\n",
    "# quantification in more detail in Tutorial 6.\n",
    "\n",
    "# Like the SklearnModel, we denote the model name as its string name (here: \"Ridge\"); this is\n",
    "# the base estimator in our ensemble. Here, we will use 100 separate Ridge models in our \n",
    "# ensemble\n",
    "\n",
    "model_ens = EnsembleModel(model='Ridge', n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's perform random 5-fold cross validation with our new ensemble model.\n",
    "\n",
    "splitter = SklearnDataSplitter(splitter='RepeatedKFold', n_repeats=1, n_splits=5)\n",
    "splitter.evaluate(X=X,\n",
    "                  y=y, \n",
    "                  models=[model_ens],\n",
    "                  preprocessor=preprocessor,\n",
    "                  metrics=metrics,\n",
    "                  plots=['Scatter', 'Histogram'],\n",
    "                  savepath=savepath,\n",
    "                  X_extra=X_extra,\n",
    "                  verbosity=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#\n",
    "# Task 3: Compare performance of scikit-learn's gradient boosting method and XGBoost\n",
    "#\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosted trees are a very popular type of regression model, and often are on par\n",
    "# or out-perform other tree-based methods. MAST-ML supports both scikit-learn's gradient\n",
    "# boosted regression model and the extreme boosting method in the XGBoost package. Let's run\n",
    "# 5-fold cross validation test on each model and see which performs better! Do either of\n",
    "# the gradient boosting methods perform better than the random forest model you ran earlier?\n",
    "#\n",
    "# I found that GBR and XGB performed about the same- with test RMSE of about 0.150 eV. Both\n",
    "# of these models outperformed the random forest from earlier- which I found to have RMSE of 0.161 eV!\n",
    "\n",
    "model_gbr = SklearnModel(model='GradientBoostingRegressor', n_estimators=150)\n",
    "model_xgb = SklearnModel(model='XGBoostRegressor', n_estimators=150)\n",
    "models = [model_gbr, model_xgb]\n",
    "\n",
    "splitter = SklearnDataSplitter(splitter='RepeatedKFold', n_repeats=1, n_splits=5)\n",
    "splitter.evaluate(X=X,\n",
    "                  y=y, \n",
    "                  models=models,\n",
    "                  preprocessor=preprocessor,\n",
    "                  metrics=metrics,\n",
    "                  plots=['Scatter', 'Histogram'],\n",
    "                  savepath=savepath,\n",
    "                  X_extra=X_extra,\n",
    "                  verbosity=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "#\n",
    "# Task 4: Compare performance of scikit-learn's neural network and Keras-based neural network regressor\n",
    "#\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to their versatility and power, neural networks are among the most commonly used\n",
    "# regression models today. MAST-ML supports creating neural networks both from scikit-learn\n",
    "# and from Keras. We already ran the scikit-learn variant earlier, and it can be created with\n",
    "# a single line call:\n",
    "\n",
    "model_nn = SklearnModel(model='MLPRegressor', hidden_layer_sizes=(20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Keras-based neural network for use in MAST-ML is a little more complicated, but can \n",
    "# still be done with just a handful of lines. Keras offers the ability to make its models function\n",
    "# like scikit-learn models, so we use that functionality here.\n",
    "\n",
    "# Import a few needed modules from Keras to build the network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# We need to define the function that builds the network architecture\n",
    "def keras_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=20, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Now we build the keras model that will be passed into MAST-ML. The build_fn parameter\n",
    "# designates the \"build function\", i.e. the function used to build the network architecture,\n",
    "# which we defined above.\n",
    "model_keras = KerasRegressor(build_fn=keras_model, epochs=100, batch_size=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've built both of our model types, we can run each of them using\n",
    "# our random 5-fold cross validation test. \n",
    "\n",
    "models = [model_nn, model_keras]\n",
    "splitter = SklearnDataSplitter(splitter='RepeatedKFold', n_repeats=1, n_splits=5)\n",
    "splitter.evaluate(X=X,\n",
    "                  y=y, \n",
    "                  models=models,\n",
    "                  preprocessor=preprocessor,\n",
    "                  metrics=metrics,\n",
    "                  plots=['Scatter', 'Histogram'],\n",
    "                  savepath=savepath,\n",
    "                  X_extra=X_extra,\n",
    "                  verbosity=3)\n",
    "\n",
    "# Which neural network performs better? In my test, it was no contest. The scikit-learn\n",
    "# MLPRegressor had a test RMSE of 0.218 eV while the Keras network had an RMSE of only 0.138 eV!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "#\n",
    "# Task 5: Compare model performance using random k-fold cross validation and leave out group cross validation\n",
    "#\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far we have only done two types of data splitter test: the most basic test where no split is\n",
    "# performed and we fit to all the data, and a random leave-out cross validation test. However,\n",
    "# many datasets have subsets of data that belong to distinct groups, and one is often interested in\n",
    "# how a model may perform when predicting new data on a brand new group. The diffusion data has\n",
    "# logical set of groups based on the host metal element, which is denoted in the 'Material compositions 1'\n",
    "# column of our data set and is designated in the groups variable when we imported the data. There are\n",
    "# 15 unique groups (metal hosts) in our current case:\n",
    "import numpy as np\n",
    "np.unique(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's focus on using a random forest model and use this model to run two tests:\n",
    "# random 5-fold cross validation and leave out group cross validation:\n",
    "\n",
    "model_rf = SklearnModel(model='RandomForestRegressor', n_estimators=150)\n",
    "splitter_random = SklearnDataSplitter(splitter='RepeatedKFold', n_repeats=1, n_splits=5)\n",
    "splitter_groups = SklearnDataSplitter(splitter='LeaveOneGroupOut')\n",
    "\n",
    "splitter_random.evaluate(X=X, \n",
    "                         y=y, \n",
    "                         models=[model_rf],\n",
    "                         preprocessor=preprocessor,\n",
    "                         metrics=metrics,\n",
    "                         plots=['Scatter', 'Histogram'],\n",
    "                         savepath=savepath,\n",
    "                         X_extra=X_extra,\n",
    "                         verbosity=3)\n",
    "\n",
    "splitter_groups.evaluate(X=X, \n",
    "                         y=y, \n",
    "                         groups=groups,\n",
    "                         models=[model_rf],\n",
    "                         preprocessor=preprocessor,\n",
    "                         metrics=metrics,\n",
    "                         plots=['Scatter', 'Histogram'],\n",
    "                         savepath=savepath,\n",
    "                         X_extra=X_extra,\n",
    "                         verbosity=3)\n",
    "\n",
    "# Which test resulted in the higher error? Why do you think this is the case?\n",
    "# I found that the leave out group splitter had a higher error with a test\n",
    "# RMSE of about 0.264 eV compared to 0.178 eV for the random cross validation test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "#\n",
    "# Task 6: Explore the limits of model performance when up to 90% of data is left out using\n",
    "#         leave out percent cross validation\n",
    "#\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As one final data splitter test in this tutorial, we can see how well (or poorly) our\n",
    "# model performs as we hold out more and more data. Typical K-fold CV only allows for \n",
    "# holding out up to 50% of the data (2-fold CV). Here, we use LeaveOutPercent to leave\n",
    "# out up to 90% of our data. Let's define several increments of different leave out percent\n",
    "# tests, then fit a Kernel ridge model to each and see how the test RMSE changes as more\n",
    "# data is left out:\n",
    "\n",
    "model_gkrr = SklearnModel(model='KernelRidge', kernel='rbf')\n",
    "\n",
    "splitter_10percent = LeaveOutPercent(percent_leave_out=0.1, n_repeats=5)\n",
    "splitter_25percent = LeaveOutPercent(percent_leave_out=0.25, n_repeats=5)\n",
    "splitter_50percent = LeaveOutPercent(percent_leave_out=0.5, n_repeats=5)\n",
    "splitter_75percent = LeaveOutPercent(percent_leave_out=0.75, n_repeats=5)\n",
    "splitter_90percent = LeaveOutPercent(percent_leave_out=0.9, n_repeats=5)\n",
    "\n",
    "splitter_10percent.evaluate(X=X,\n",
    "                            y=y, \n",
    "                            models=[model_gkrr],\n",
    "                            preprocessor=preprocessor,\n",
    "                            metrics=metrics,\n",
    "                            plots=['Scatter', 'Histogram'],\n",
    "                            savepath=savepath,\n",
    "                            X_extra=X_extra,\n",
    "                            verbosity=3)\n",
    "splitter_25percent.evaluate(X=X,\n",
    "                            y=y, \n",
    "                            models=[model_gkrr],\n",
    "                            preprocessor=preprocessor,\n",
    "                            metrics=metrics,\n",
    "                            plots=['Scatter', 'Histogram'],\n",
    "                            savepath=savepath,\n",
    "                            X_extra=X_extra,\n",
    "                            verbosity=3)\n",
    "splitter_50percent.evaluate(X=X,\n",
    "                            y=y, \n",
    "                            models=[model_gkrr],\n",
    "                            preprocessor=preprocessor,\n",
    "                            metrics=metrics,\n",
    "                            plots=['Scatter', 'Histogram'],\n",
    "                            savepath=savepath,\n",
    "                            X_extra=X_extra,\n",
    "                            verbosity=3)\n",
    "splitter_75percent.evaluate(X=X,\n",
    "                            y=y, \n",
    "                            models=[model_gkrr],\n",
    "                            preprocessor=preprocessor,\n",
    "                            metrics=metrics,\n",
    "                            plots=['Scatter', 'Histogram'],\n",
    "                            savepath=savepath,\n",
    "                            X_extra=X_extra,\n",
    "                            verbosity=3)\n",
    "splitter_90percent.evaluate(X=X,\n",
    "                            y=y, \n",
    "                            models=[model_gkrr],\n",
    "                            preprocessor=preprocessor,\n",
    "                            metrics=metrics,\n",
    "                            plots=['Scatter', 'Histogram'],\n",
    "                            savepath=savepath,\n",
    "                            X_extra=X_extra,\n",
    "                            verbosity=3)\n",
    "\n",
    "# How does the test RMSE change as more data is left out? For my case, I found the following:\n",
    "#    10%: 0.159 eV \n",
    "#    25%: 0.151 eV\n",
    "#    50%: 0.179 eV\n",
    "#    75%: 0.213 eV\n",
    "#    90%: 0.286 eV\n",
    "#\n",
    "# Not surprisingly, the model becomes worse as less data is used for training. The minimum amount\n",
    "# of data needed to obtain what constitutes good model performance will be application specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You've now completed your fourth MAST-ML tutorial notebook! Now that you're more familiar with working\n",
    "# with different types of models and cross validation tests to evaluate those models, we are ready to \n",
    "# move on to some more advanced fitting methods, like nested cross validation and model optimization\n",
    "#\n",
    "# The next example in this notebook series is titled MASTML_Tutorial_5_NestedCV_and_OptimizedModels.ipynb, \n",
    "# and will guide you through the process of assessing model performance on true test data, and generating the\n",
    "# best optimized model for a specified cross validation test"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMEVkHt3gXnG4BXBNcr+orE",
   "name": "MASTML_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
