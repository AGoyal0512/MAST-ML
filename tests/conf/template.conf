# You run this with `$ python mastml.py settings.conf data.csv -o results/`
# Second example:   `$ python mastml.py input.conf compositions.csv -o Desktop/model-results/`

# Or you open the website, upload a csv, upload a conf file, and download the resulting zip file

# Sections and subsections are in CamelCase; parameters are in snake_case


[GeneralSetup]
    input_features = Auto # Defaults to all but last column (specifying Auto is same as omiting this option)
    #input_features = square_footage, crime_rate, year_built # you can specify which columns from the csv you'd like to keep
    target_feature = Auto # Defaults to last column
    #randomizer = true # set true for randomly shuffly y rows


[FeatureNormalization]
    [[MeanStdevScaler]]

    [[MinMaxScaler]]

[FeatureGeneration] # If you don't want to use a particular API, then omit or comment out that subsection.
                    # If you don't want to do any feature generation, then omit this entire section
    #TODO: add all sklearn feature generation routines, plus our own
    [[Magpie]]
        api_key = 1234

    [[MaterialsProject]]
        api_key = 1234

    [[Citrine]]
        api_key=1234

    #[[custom]]
    #    area = length * width # create new columns in the dataframe using algebra on existing columns

    [[PolynomialFeatures]]

    [[ContainsElement]] # generate a new column with 1 or 0 for contains element
        composition_feature = MaterialComposition # name of column containing material composition
        element = C # For carbon, for example
        new_name = compositions # the name of the column in the csv containing the compositions



[FeatureSelection]
    #regression scoring functions: f_regression, mutual_info_regression
    #classification scoring functions: chi2, f_classif, mutual_info_classif
    #TODO: add all sklearn feature selection routines, plus our own

    [[RemoveConstantFeatures]]
    [[PrincipleComponentAnalysis]]
    [SelectKBest]
        k = 3
        scoring = f_classif
    [[VarianceThreshold]]
        threshold = 0.2
    [[SelectPercentile]]
        percentile = 10 # percentage of features to keep
        scoring = chi2
    [[SelectFpr]] # Select features based on a false positive rate test.
    [[SelectFdr]] # Select features based on an estimated false discovery rate.
    [[SelectFwe]] # Select features based on family-wise error rate.
    [[GenericUnivariateSelect]] # Univariate feature selector with configurable mode.
    [[RFE]] # recursive feature elimination


[DataSplits] # mastml will train & test the model on each of the following data splits.
             # Omit a split to not use it.
             # Or list a split without args to use it with default args.
             # Almost every option has a default argument.

    #TODO: add all sklearn data split routines, plus our own

    # Classification metrics: accuracy, average_precision, f1, f1_micro, f1_macro, f1_weighted, f1_samples, neg_log_loss, precision, recall, roc_auc,
    # Regression metrics: explained_variance, neg_mean_absolute_error, neg_mean_squared_error, neg_mean_squared_log_error, neg_median_absolute_error, r2,

    [[NoSplit]] # Just train the model on the training data and test it on that same data

    #[[Randomize]] # Randomly remap x and y values (just to see how the model does with nonsense input

    #[[KFoldRandomize]] # First randomize all the x-y matchings, then see how well the nonsense-model does on cross validation

    [[JustEachGroup]] # Train the model on one group at a time and test it on the rest of the data
        grouping_features = class # The name of the column in the csv which contains classes

    [[WithoutEachGroup]] # Train the model on (n-1)/n groups and test on the excluded group
        grouping_feature = class
        plots = avg_vs_worst

# MARK
    [[KFold]] # Split the data into k even chunks. Try training/testing on each chunk.
        k = 5

    [[LeaveOutPercent]] # Like KFold but percentage
        percentage = 20

    [[LeaveOneOut]] # Try training the model on all the data except one row, for every row


[Models] # List the sklearn models you want to use and the parameters you want to change.
         # Your models should be either all classifiers or all regressors. No mixing allowed.
         # All parameters have default settings. If you ommit a parameter, then the default setting is used.
         # Go to http://scikit-learn.org/stable/documentation.html and search for details on a particular model.
         # A single conf file should use zero or more models.
         # mastml will run each enabled split with each enabled model
         # Also check this out:
         # http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
     ### REGRESSORS

    [[SVR]]
        C
        cache_size
        coef0
        degree
        epsilon
        gamma
        kernel = rbf # or try: linear
        max_iter
        shrinking


    [[Lasso]]
        alpha
        copy_X
        fit_intercept
        max_iter
        normalize
        positive
        precompute
        random_state
        selection
        tol
        warm_start


    [[ElasticNet]]
        alpha
        copy_X
        fit_intercept
        l1_ratio
        max_iter
        normalize
        positive
        precompute
        random_state
        selection
        tol
        warm_start


    [[Ridge]]
        alpha
        copy_X
        fit_intercept
        max_iter
        normalize
        random_state
        solver
        tol


    [[LinearRegression]]
        copy_X
        fit_intercept
        n_jobs
        normalize

    [[DecisionTreeRegressor]]
        criterion
        max_depth
        max_features
        max_leaf_nodes
        min_impurity_decrease
        min_impurity_split
        min_samples_leaf
        min_samples_split
        min_weight_fraction_leaf
        presort
        random_state
        splitter

    [[KernelRidge]]
        alpha
        coef0
        degree
        gamma
        kernel
        kernel_params


    ### Classifiers

    [[MLPClassifier]]
        activation
        alpha
        batch_size
        beta_1
        beta_2
        early_stopping
        epsilon
        hidden_layer_sizes
        learning_rate
        learning_rate_init
        max_iter
        momentum
        nesterovs_momentum
        power_t
        random_state
        shuffle
        solver
        tol
        validation_fraction
        verbose
        warm_start


    [[KNeighborsClassifier]]
        algorithm
        leaf_size
        metric
        metric_params
        n_jobs
        n_neighbors
        p
        weights



    [[SVC]]
        C
        cache_size
        class_weight
        coef0
        decision_function_shape
        degree
        gamma
        kernel
        max_iter
        probability
        random_state
        shrinking
        tol
        verbose


    [[GaussianProcessClassifier]]
        copy_X_train
        kernel
        max_iter_predict
        multi_class
        n_jobs
        n_restarts_optimizer
        optimizer
        random_state
        warm_start


    [[DecisionTreeClassifier]]
        class_weight
        criterion
        max_depth
        max_features
        max_leaf_nodes
        min_impurity_decrease
        min_impurity_split
        min_samples_leaf
        min_samples_split
        min_weight_fraction_leaf
        presort
        random_state
        splitter


    [[RandomForestClassifier]]
        bootstrap
        class_weight
        criterion
        max_depth
        max_features
        max_leaf_nodes
        min_impurity_decrease
        min_impurity_split
        min_samples_leaf
        min_samples_split
        min_weight_fraction_leaf
        n_estimators
        n_jobs
        oob_score
        random_state
        verbose
        warm_start


    [[AdaBoostClassifier]]
        algorithm
        base_estimator
        learning_rate
        n_estimators
        random_state


    [[QuadraticDiscriminantAnalysis]]
        priors
        reg_param
        store_covariance
        store_covariances
        tol


[PlotSettings]
    target_histogram = True
    train_test_plots = True
    predicted_vs_true = True
    predicted_vs_true_bars = True
    best_worst_per_point = True

