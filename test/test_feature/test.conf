[General Setup]

    save_path = ./
    input_features = Auto #time, N_sine_feature,N_linear_feature
    target_feature = y_feature
    target_error_feature = N_linear_feature #need to change
    labeling_features = str_cat, num_id
    grouping_feature = str_cat
    normalize_features = True

#[CSV Setup]
#    setup_class = test.random_data.MakeRandomData
#    save_path = ../random_data
#    random_seed = 0

[Feature Generation] #optional
    add_magpie_features = True
    add_materialsproject_features = False
    materialsproject_apikey = 'TtAHFCrZhQa7cwEy' # Use your own Materials Project API key
    #add_citrine_features = False #This won't work right now
    #citrine_apikey = 'amQVQutFrr7etr4ufQQh0gtt' # Use your own Citrine API key
    
[Feature Selection]
    remove_constant_features = False
    feature_selection_algorithm = forward #Must be "forward" for now 6/26/17
    selection_type = regression # Don't specify for now 6/26/17
    number_of_features_to_keep = 10

[Data Setup]

    [[Initial]]
    data_path = ../random_data/random_test_data_with_comps.csv
    weights = False #haven't tested weighting; may move


[Models and Tests to Run]

    models = gkrr_model
    test_cases = ParamOptGA, SingleFit_fromparams

[Test Parameters]

    [[ParamOptGA]]
    training_dataset = Initial
    testing_dataset = Initial #,Set2
    num_folds = 2  #todo: also allow percent_leave_out = 50 for example
    num_cvtests = 20
    num_gas = 2
    population_size = 5
    convergence_generations = 2
    max_generations = 8
    fix_random_for_testing = 1
    num_parents = 3
    use_multiprocessing = 2 #only for cluster; races on Mac?
    additional_feature_methods = Testing.subtraction;2;col1:N_sine_feature;col2:N_linear_feature
    #additional_feature_methods = DBTT.calculate_EffectiveFluence;1;flux_feature:N_sine_feature;fluence_feature:N_linear_feature
    
    
    [[SingleFit_fromparams]]
    training_dataset = Initial
    testing_dataset  = Initial
    xlabel = Measured target
    ylabel = Target prediction
    stepsize = 1.0

[Model Parameters]

    [[linear_model]]
    fit_intercept = True

    [[decision_tree_model]]
    max_depth = 5
    min_samples_split = 2
    min_samples_leaf = 1
    splitter = best
    criterion = mse

    [[gkrr_model]]
    alpha = 0.00139
    coef0 = 1
    degree = 3
    gamma = 0.518
    kernel = rbf

    [[lkrr_model]]
    alpha = 0.00518
    gamma = 0.518
    kernel = laplacian

    [[randomforest_model]]
    criterion = mse
    n_estimators = 100
    max_depth = 5
    min_samples_split = 2
    min_samples_leaf = 1
    max_leaf_nodes = 2
    n_jobs = 1
    
    [[nn_model]]
    hidden_layer_sizes = 10 #Only single size supported in hyperparam optimization
    activation = relu # Choose relu, tanh, logistic, identity
    solver = adam # Choose adam, lbfgs, sgd
    alpha = 0.0001
    max_iter = 200
    tol = 0.0001 #Not optimized in hyperparameter optimization

    [[adaboost_model]]
    estimators = 275
    max_depth = 12
    min_samples_split = 2
    min_samples_leaf = 1
    learning rate = 1
    loss function = linear
