[Data Setup]

    data_path = ../random_data/random_test_data.csv
    save_path = ./
    #lwr_data_path = CD_LWR_clean8.csv
    #X = N(Cu), N(Ni), N(Mn), N(P), N(Si), N( C ), N(log(fluence), N(log(flux), N(Temp)
    X = time,N_sine_feature,N_linear_feature
    y = y_feature
    weights = False # whether or not the weights column in the dataset is applied which duplicates weighted data

[Models and Tests to Run]

    models = gkrr_model
    test_cases = AnalysisTemplate_withy,AnalysisTemplate_noy

#if some test files have different configuration setting than AllTests, you can make changes by adding a
#separate section
[Test Parameters]
    [[AnalysisTemplate_withy]]
    training_csv=../random_data/random_test_data.csv
    testing_csv=../random_data/random_test_data.csv
    input_features=time, N_sine_feature, N_linear_feature
    target_feature=y_feature
    #save_path=./
    analysis_name=template_test

    [[AnalysisTemplate_noy]]
    training_csv=../random_data/random_test_data.csv
    testing_csv=../random_data/random_test_data_noy.csv
    input_features=time, N_sine_feature, N_linear_feature
    target_feature=y_feature
    #save_path=./
    analysis_name=template_test_no_y


[Model Parameters]

    [[linear_model]]
    fit_intercept = True

    [[decision_tree_model]]
    max_depth = 5
    min_samples_split = 2
    min_samples_leaf = 1
    split_criterion = mse

    [[gkrr_model]]
    alpha = 0.00139
    coef0 = 1
    degree = 3
    gamma = 0.518
    kernel = rbf

    [[lkrr_model]]
    alpha = 0.00518
    gamma = 0.518
    kernel = laplacian

    [[randomforest_model]]
    split_criterion = mse
    estimators = 100
    max_depth = 5
    min_samples_split = 2
    min_samples_leaf = 1
    max_leaf_nodes = 2
    jobs = 1

    [[adaboost_model]]
    estimators = 275
    max_depth = 12
    min_samples_split = 2
    min_samples_leaf = 1
    learning rate = 1
    loss function = linear

    #minmax, size, transfer_function are the verbatim arguments for neurolab.net.newff()
    #training_algorithm is the verbatim 'support train fcn' for neurolab.train omitting 'train_'
    #see: https://pythonhosted.org/neurolab/lib.html#module-neurolab.net
    #epochs,show,goal are neurolab.net.train() arguments
    #see: https://pythonhosted.org/neurolab/lib.html#train-algorithms-based-gradients-algorithms
    #NOTE: minmax is verbose b/c [[0,1]]*9 will have bad pointers
    [[nn_model_neurolab]]
    #minmax = [[0, 1],[0, 1],[0, 1],[0, 1],[0, 1],[0, 1],[0, 1],[0, 1],[0, 1]]
    minmax = [0, 1], [0, 1], [0, 1]
    size = 11, 1
    transfer_function = TanSig
    training_algorithm = bfgs
    epochs = 5
    show = False
    goal = 0.01[default]
